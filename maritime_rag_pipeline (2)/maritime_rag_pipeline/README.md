# Maritime RAG Pipeline with QLoRA Fine‑Tuning

This repository contains all of the pieces you need to build a retrieval‑augmented generation (RAG) system on top of your maritime traffic dataset and then fine‑tune a language model using QLoRA.  The goal is to provide a reproducible project that you can run end‑to‑end: from data preparation, through vector storage, instruction generation, to model fine‑tuning and evaluation.

## Folder Structure

- `data/`
  - `final_data.csv` – **Place your preprocessed maritime events CSV here.**  This file is not provided in the repository because it is large; you need to copy your cleaned dataset into this location before running the pipeline.
  - `faiss_docs_export.csv` – Optional: an export of your existing FAISS index if you already built one.  Not required for a fresh run.

- `scripts/`
  - `chunk_data.py` – Reads the cleaned dataset, converts each row into a text string, splits the texts into overlapping chunks, embeds each chunk using a sentence‑transformer model and stores them in a FAISS vector store.  The resulting index is saved under `vector_db/`.
  - `generate_instructions.py` – Generates synthetic instruction–response pairs from the cleaned data.  Uses random selection and simple logic to produce questions about vessel arrivals, dwell times, etc.  Outputs a JSONL file ready for supervised fine‑tuning.
  - `fine_tune_qlora.py` – A skeleton script for fine‑tuning a Llama 3.1–style model using QLoRA.  It loads your synthetic instruction dataset and trains a PEFT adapter on top of the base model.  This script is configured for low‑resource machines; adjust batch sizes and epochs as needed.
  - `evaluate_retrieval.py` – Evaluates the retrieval pipeline (FAISS + embeddings) by running a set of test queries and checking recall/precision of the returned chunks.
  - `evaluate_llm.py` – Evaluates the fine‑tuned model on a held‑out set of queries.  Uses the retrieval pipeline to fetch context and the model to produce answers.

- `vector_db/`
  - This folder is created automatically by `chunk_data.py` when you build the FAISS index.  It contains the FAISS files (`index.faiss` and `index.pkl`) and metadata used by LangChain.

- `training_data/`
  - `instructions.jsonl` – The synthetic instruction dataset generated by `generate_instructions.py`.  You can adjust the number of examples and templates in the script.
  - `eval_questions.json` – An example of a held‑out evaluation set for the LLM and retrieval pipeline.

- `requirements.txt` – Python dependencies needed to run the scripts.  Install these into a virtual environment with `pip install -r requirements.txt`.

## Getting Started

1. **Set up your environment**
   ```bash
   # Create and activate a virtual environment (optional but recommended)
   python -m venv .venv
   source .venv/bin/activate  # on Windows use .venv\Scripts\activate

   # Install dependencies
   pip install -r requirements.txt
   ```

2. **Prepare your data**
   - Copy your cleaned maritime CSV (e.g. `final_data.csv`) into the `data/` directory.  Ensure it has at least the columns `portName`, `portArrival`, `portDeparture` and `ais_VesselType`.
   - If you already have a FAISS index you want to reuse, place `faiss_docs_export.csv` in the same directory.

3. **Build the vector store**
   ```bash
   python scripts/chunk_data.py --input data/final_data.csv --output vector_db --sample_rows 0 \
       --chunk_size 800 --chunk_overlap 150 --embedding_model sentence-transformers/all-MiniLM-L6-v2
   ```
   - `--sample_rows 0` means use the entire CSV.  To test quickly, set this to a smaller number (e.g. 20000).
   - The script will create `vector_db/` with `index.faiss` and `index.pkl`.

4. **Generate synthetic instructions**
   ```bash
   python scripts/generate_instructions.py --input data/final_data.csv --output training_data/instructions.jsonl \
       --num_samples 20000
   ```
   - Adjust `--num_samples` to control the size of your fine‑tuning dataset.  10k–30k examples is typical.

5. **Fine‑tune the model with QLoRA**
   ```bash
   python scripts/fine_tune_qlora.py --dataset training_data/instructions.jsonl --model meta-llama/Llama-3-8b-instruct \
       --output_dir training_data/qlora-adapter --epochs 2 --batch_size 2 --learning_rate 2e-4
   ```
   - This will load the base Llama 3.1 8B model, attach a QLoRA adapter and fine‑tune on your synthetic data.  The resulting adapter is saved in `training_data/qlora-adapter`.

6. **Evaluate your system**
   - **Retrieval evaluation:**
     ```bash
     python scripts/evaluate_retrieval.py --index_dir vector_db --eval_set training_data/eval_questions.json
     ```
   - **LLM evaluation:**
     ```bash
     python scripts/evaluate_llm.py --index_dir vector_db --adapter_dir training_data/qlora-adapter --eval_set training_data/eval_questions.json \
         --model meta-llama/Llama-3-8b-instruct
     ```

The evaluation scripts report recall and simple metrics.  You should extend them to compute task‑specific metrics (e.g. accuracy of counts or dwell time predictions).

## Notes

- **Hardware requirements:** Fine‑tuning a 8B parameter model with QLoRA still requires a GPU with at least 16 GB of VRAM.  If you lack a GPU, consider using a smaller model (e.g. Llama‑2 7B) or using a cloud service.
- **Data privacy:** The synthetic instruction generator uses random ports and times drawn from your dataset.  No sensitive information is included.  If you need more realistic questions, edit the templates in `generate_instructions.py`.
- **Limitations:** The provided templates and evaluation are minimal examples.  You should expand the coverage of your instructions to include congestion prediction, CO₂ estimation (after enriching your data), and other domain‑specific tasks.

Feel free to customise and extend this repository.  The scripts are annotated and designed to be easy to modify.  If you encounter issues, please read the code comments and adjust parameters accordingly.